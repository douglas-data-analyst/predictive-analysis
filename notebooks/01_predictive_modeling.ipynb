{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise Preditiva de Vendas com Scikit-Learn\n",
    "\n",
    "Este notebook implementa um modelo de análise preditiva para prever vendas futuras e identificar fatores que influenciam o desempenho de produtos em um e-commerce, utilizando técnicas de machine learning com scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importação de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Importações\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "# Ignorando avisos para melhor visualização\n",
    "warnings.filterwarnings("ignore")\n",
    "\n",
    "# Adicionando diretório src ao path para importar módulos personalizados\n",
    "sys.path.append("../src")\n",
    "import modeling_utils as mu\n",
    "\n",
    "# Importações de Scikit-Learn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Configurações de visualização\n",
    "plt.style.use("seaborn-v0_8-whitegrid")\n",
    "sns.set_palette("viridis")\n",
    "pd.set_option("display.max_columns", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Carregamento e Preparação dos Dados\n",
    "\n",
    "Nesta seção, carregamos os dados processados do projeto ETL e realizamos a preparação final para a modelagem preditiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Definindo função para criar dados de exemplo se os arquivos não existirem\n",
    "def create_sample_processed_data():\n",
    "    print(\"Criando dados de exemplo processados para demonstração...\")\n",
    "    os.makedirs("../data/processed", exist_ok=True)\n",
    "    \n",
    "    # Dados de exemplo para fact_sales\n",
    "    n_samples = 1000\n",
    "    fact_sales_data = {\n",
    "        "order_id": [f"order_{i}" for i in range(n_samples)],\n",
    "        "order_item_id": np.random.randint(1, 3, n_samples),\n",
    "        "product_id": [f"prod_{np.random.randint(1, 100)}" for _ in range(n_samples)],\n",
    "        "seller_id": [f"seller_{np.random.randint(1, 50)}" for _ in range(n_samples)],\n",
    "        "customer_id": [f"cust_{np.random.randint(1, 200)}" for _ in range(n_samples)],\n",
    "        "date_id": pd.to_datetime(pd.date_range(start="2022-01-01", periods=n_samples, freq="D")).strftime("%Y%m%d").astype(int),\n",
    "        "price": np.random.uniform(10, 500, n_samples).round(2),\n",
    "        "freight_value": np.random.uniform(5, 50, n_samples).round(2),\n",
    "        "review_score": np.random.randint(1, 6, n_samples)\n",
    "    }\n",
    "    fact_sales_df = pd.DataFrame(fact_sales_data)\n",
    "    fact_sales_df.to_parquet("../data/processed/fact_sales.parquet")\n",
    "    \n",
    "    # Dados de exemplo para dim_date\n",
    "    date_range = pd.date_range(start="2022-01-01", periods=n_samples, freq="D")\n",
    "    dim_date_data = {\n",
    "        "id": date_range.strftime("%Y%m%d").astype(int),\n",
    "        "date": date_range,\n",
    "        "year": date_range.year,\n",
    "        "month": date_range.month,\n",
    "        "day": date_range.day,\n",
    "        "dayofweek": date_range.dayofweek,\n",
    "        "quarter": date_range.quarter,\n",
    "        "is_weekend": date_range.dayofweek.isin([5, 6]).astype(int)\n",
    "    }\n",
    "    dim_date_df = pd.DataFrame(dim_date_data)\n",
    "    dim_date_df.to_parquet("../data/processed/dim_date.parquet")\n",
    "    \n",
    "    # Dados de exemplo para dim_product\n",
    "    dim_product_data = {\n",
    "        "id": [f"prod_{i}" for i in range(1, 101)],\n",
    "        "product_category_name_english": np.random.choice(["electronics", "furniture", "toys", "books", "clothing"], 100),\n",
    "        "product_weight_g": np.random.uniform(100, 5000, 100),\n",
    "        "product_photos_qty": np.random.randint(1, 5, 100)\n",
    "    }\n",
    "    dim_product_df = pd.DataFrame(dim_product_data)\n",
    "    dim_product_df.to_parquet("../data/processed/dim_product.parquet")\n",
    "    \n",
    "    print(\"Dados de exemplo processados criados com sucesso!\")\n",
    "\n",
    "# Verificando se os arquivos processados existem\n",
    "required_processed_files = [\n",
    "    "../data/processed/fact_sales.parquet",\n",
    "    "../data/processed/dim_date.parquet",\n",
    "    "../data/processed/dim_product.parquet"\n",
    "]\n",
    "\n",
    "if not all(os.path.exists(file) for file in required_processed_files):\n",
    "    create_sample_processed_data()\n",
    "\n",
    "# Carregando dados processados\n",
    "print(\"Carregando dados processados...\")\n",
    "fact_sales = pd.read_parquet("../data/processed/fact_sales.parquet")\n",
    "dim_date = pd.read_parquet("../data/processed/dim_date.parquet")\n",
    "dim_product = pd.read_parquet("../data/processed/dim_product.parquet")\n",
    "\n",
    "# Juntando tabelas para criar o dataset de modelagem\n",
    "df_model = pd.merge(fact_sales, dim_date, left_on="date_id", right_on="id", suffixes=("", "_date"))\n",
    "df_model = pd.merge(df_model, dim_product, left_on="product_id", right_on="id", suffixes=("", "_product"))\n",
    "\n",
    "# Selecionando colunas relevantes e renomeando\n",
    "df_model = df_model[[\n",
    "    "date", "price", "freight_value", "review_score", "year", "month", "day", "dayofweek", "quarter", "is_weekend",\n",
    "    "product_category_name_english", "product_weight_g", "product_photos_qty"\n",
    "]].copy()\n",
    "\n",
    "df_model.rename(columns={"product_category_name_english": "category"}, inplace=True)\n",
    "\n",
    "# Agregando dados por dia para prever vendas diárias\n",
    "df_daily_sales = df_model.groupby("date").agg(\n",
    "    total_sales=("price", "sum"),\n",
    "    avg_freight=("freight_value", "mean"),\n",
    "    avg_review_score=("review_score", "mean"),\n",
    "    year=("year", "first"),\n",
    "    month=("month", "first"),\n",
    "    day=("day", "first"),\n",
    "    dayofweek=("dayofweek", "first"),\n",
    "    quarter=("quarter", "first"),\n",
    "    is_weekend=("is_weekend", "first")\n",
    ").reset_index()\n",
    "\n",
    "# Ordenando por data\n",
    "df_daily_sales = df_daily_sales.sort_values("date").set_index("date")\n",
    "\n",
    "print(\"Dataset para modelagem criado:\")\n",
    "display(df_daily_sales.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Engenharia de Features\n",
    "\n",
    "Criamos features adicionais que podem ajudar o modelo a capturar padrões nos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Criando features de lag (vendas passadas)\n",
    "for lag in [1, 7, 14, 30]:\n",
    "    df_daily_sales[f"sales_lag_{lag}"] = df_daily_sales["total_sales"].shift(lag)\n",
    "\n",
    "# Criando features de média móvel\n",
    "for window in [7, 14, 30]:\n",
    "    df_daily_sales[f"sales_rolling_mean_{window}"] = df_daily_sales["total_sales"].shift(1).rolling(window=window).mean()\n",
    "    df_daily_sales[f"sales_rolling_std_{window}"] = df_daily_sales["total_sales"].shift(1).rolling(window=window).std()\n",
    "\n",
    "# Removendo linhas com NaN geradas pelas features de lag e média móvel\n",
    "df_daily_sales = df_daily_sales.dropna()\n",
    "\n",
    "print(\"Dataset com features de engenharia:\")\n",
    "display(df_daily_sales.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Definição de Features (X) e Target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Definindo features (X) e target (y)\n",
    "X = df_daily_sales.drop("total_sales", axis=1)\n",
    "y = df_daily_sales["total_sales"]\n",
    "\n",
    "# Identificando features numéricas e categóricas (neste caso, não temos categóricas explícitas após agregação)\n",
    "# Se tivéssemos categorias como "category" no df_daily_sales, elas seriam tratadas aqui.\n",
    "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include="object").columns.tolist() # Exemplo, pode estar vazia\n",
    "\n",
    "print(f"Features numéricas: {numerical_features}")\n",
    "print(f"Features categóricas: {categorical_features}")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Divisão dos Dados em Treino e Teste\n",
    "\n",
    "Para séries temporais, é importante dividir os dados de forma cronológica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Divisão cronológica dos dados (80% treino, 20% teste)\n",
    "split_point = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_point], X[split_point:]\n",
    "y_train, y_test = y[:split_point], y[split_point:]\n",
    "\n",
    "print(f"Tamanho do conjunto de treino: {X_train.shape[0]} amostras")\n",
    "print(f"Tamanho do conjunto de teste: {X_test.shape[0]} amostras")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Criação do Pipeline de Pré-processamento e Modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Pipeline de pré-processamento\n",
    "# Para features numéricas: StandardScaler\n",
    "# Para features categóricas: OneHotEncoder (se houver)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ("num", StandardScaler(), numerical_features),\n",
    "        # ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_features) # Descomentar se houver features categóricas\n",
    "    ], remainder="passthrough" # Mantém colunas não especificadas (se houver)\n",
    ")\n",
    "\n",
    "# Definindo modelos a serem testados\n",
    "models = {\n",
    "    "Linear Regression": LinearRegression(),\n",
    "    "Ridge Regression": Ridge(),\n",
    "    "Random Forest": RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "    "Gradient Boosting": GradientBoostingRegressor(random_state=42),\n",
    "    "XGBoost": xgb.XGBRegressor(random_state=42, n_jobs=-1),\n",
    "    "LightGBM": lgb.LGBMRegressor(random_state=42, n_jobs=-1)\n",
    "}\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Treinamento e Avaliação dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Treinando e avaliando cada modelo\n",
    "for model_name, model in models.items():\n",
    "    print(f"\\nTreinando {model_name}...")\n",
    "    \n",
    "    # Criando o pipeline completo\n",
    "    pipeline = Pipeline(steps=[("preprocessor", preprocessor), ("regressor", model)])\n",
    "    \n",
    "    # Treinando o modelo\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Fazendo previsões\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Avaliando o modelo\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results[model_name] = {"RMSE": rmse, "MAE": mae, "R2": r2}\n",
    "    \n",
    "    print(f"{model_name} - RMSE: {rmse:.2f}, MAE: {mae:.2f}, R2: {r2:.2f}")\n",
    "\n",
    "# Exibindo resultados\n",
    "results_df = pd.DataFrame(results).T.sort_values(by="R2", ascending=False)\n",
    "print(\"\\nResultados da Avaliação dos Modelos:\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualização das Previsões do Melhor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Selecionando o melhor modelo (baseado no R2, por exemplo)\n",
    "best_model_name = results_df.index[0]\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f"Melhor modelo: {best_model_name}")\n",
    "\n",
    "# Criando e treinando o pipeline do melhor modelo\n",
    "best_pipeline = Pipeline(steps=[("preprocessor", preprocessor), ("regressor", best_model)])\n",
    "best_pipeline.fit(X_train, y_train)\n",
    "y_pred_best = best_pipeline.predict(X_test)\n",
    "\n",
    "# Criando DataFrame para visualização\n",
    "predictions_df = pd.DataFrame({\n",
    "    "Data": X_test.index,\n",
    "    "Vendas Reais": y_test,\n",
    "    "Vendas Previstas": y_pred_best\n",
    "}).set_index("Data")\n",
    "\n",
    "# Visualizando previsões vs. valores reais\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(predictions_df.index, predictions_df["Vendas Reais"], label="Vendas Reais", color="blue", alpha=0.7)\n",
    "plt.plot(predictions_df.index, predictions_df["Vendas Previstas"], label="Vendas Previstas", color="red", linestyle="--")\n",
    "plt.title(f"Previsões de Vendas vs. Valores Reais ({best_model_name})", fontsize=16)\n",
    "plt.xlabel("Data", fontsize=12)\n",
    "plt.ylabel("Vendas Totais (R$)", fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "os.makedirs("../reports/figures", exist_ok=True)\n",
    "plt.savefig("../reports/figures/model_performance.png", dpi=300, bbox_inches="tight")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Análise de Importância das Features (para modelos baseados em árvores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verificando se o melhor modelo tem o atributo feature_importances_\n",
    "if hasattr(best_pipeline.named_steps["regressor"], "feature_importances_"):\n",
    "    # Obtendo nomes das features após o pré-processamento\n",
    "    # Isso pode ser complexo dependendo do pré-processador (ex: OneHotEncoder cria novas colunas)\n",
    "    # Para este exemplo simplificado, usaremos as colunas originais de X\n",
    "    # Em um cenário real, você precisaria obter os nomes das features transformadas\n",
    "    try:\n",
    "        feature_names = best_pipeline.named_steps["preprocessor"].get_feature_names_out()\n",
    "    except AttributeError: # Para versões mais antigas do scikit-learn ou transformadores sem get_feature_names_out\n",
    "        feature_names = X.columns # Aproximação, pode não ser precisa com OneHotEncoder\n",
    "        \n",
    "    importances = best_pipeline.named_steps["regressor"].feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({"Feature": feature_names, "Importance": importances})\n",
    "    feature_importance_df = feature_importance_df.sort_values(by="Importance", ascending=False).head(10)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x="Importance", y="Feature", data=feature_importance_df, palette="viridis")\n",
    "    plt.title(f"Top 10 Features Mais Importantes ({best_model_name})", fontsize=16)\n",
    "    plt.xlabel("Importância", fontsize=12)\n",
    "    plt.ylabel("Feature", fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig("../reports/figures/feature_importance.png", dpi=300, bbox_inches="tight")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f"O modelo {best_model_name} não suporta diretamente a análise de importância de features (ex: Linear Regression).")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Salvando o Melhor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Salvando o pipeline do melhor modelo\n",
    "model_save_path = "../data/models/best_sales_prediction_model.joblib"\n",
    "os.makedirs("../data/models", exist_ok=True)\n",
    "joblib.dump(best_pipeline, model_save_path)\n",
    "print(f"Melhor modelo salvo em: {model_save_path}")\n",
    "\n",
    "# Exemplo de como carregar o modelo salvo\n",
    "# loaded_model = joblib.load(model_save_path)\n",
    "# y_pred_loaded = loaded_model.predict(X_test)\n",
    "# print(f\"RMSE do modelo carregado: {np.sqrt(mean_squared_error(y_test, y_pred_loaded)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusão\n",
    "\n",
    "Neste notebook, implementamos um pipeline completo de análise preditiva para prever vendas diárias de e-commerce, incluindo:\n",
    "\n",
    "1. **Carregamento e Preparação dos Dados**: Combinamos dados de vendas, datas e produtos.\n",
    "2. **Engenharia de Features**: Criamos features de lag e médias móveis para capturar tendências temporais.\n",
    "3. **Divisão dos Dados**: Separamos os dados em conjuntos de treino e teste de forma cronológica.\n",
    "4. **Modelagem**: Treinamos e avaliamos múltiplos algoritmos de regressão (Linear Regression, Ridge, Random Forest, Gradient Boosting, XGBoost, LightGBM).\n",
    "5. **Seleção do Melhor Modelo**: Identificamos o modelo com melhor desempenho com base em métricas como RMSE, MAE e R2.\n",
    "6. **Visualização das Previsões**: Comparamos as previsões do melhor modelo com os valores reais.\n",
    "7. **Análise de Importância das Features**: Identificamos as features mais influentes para modelos baseados em árvores.\n",
    "8. **Salvamento do Modelo**: Salvamos o pipeline do melhor modelo para uso futuro.\n",
    "\n",
    "O modelo desenvolvido pode ser utilizado para prever vendas futuras, auxiliar no planejamento de estoque e otimizar estratégias de marketing. Próximos passos poderiam incluir otimização de hiperparâmetros mais robusta (ex: Bayesian Optimization), validação cruzada para séries temporais mais avançada (ex: TimeSeriesSplit) e deployment do modelo em um ambiente de produção."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
